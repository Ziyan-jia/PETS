{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Pets nn .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNBZOKwZ2cjj7IKqhGa2XCP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ziyan-jia/PETS/blob/main/Copy_of_Pets_nn_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hZJECYec7i8l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df979d3a-a9f7-43b0-e23f-cd992c717a87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpi4py\n",
            "  Downloading mpi4py-3.1.3.tar.gz (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 6.7 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.1.3-cp37-cp37m-linux_x86_64.whl size=2185293 sha256=da4b6d32e205bd3ba9ec386824ab5a28150909a204e7c49d4400d5d8c545ee9d\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/07/14/6a0c63fa2c6e473c6edc40985b7d89f05c61ff25ee7f0ad9ac\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-3.1.3\n",
            "Collecting mbrl\n",
            "  Downloading mbrl-0.1.5-py3-none-any.whl (134 kB)\n",
            "\u001b[K     |████████████████████████████████| 134 kB 8.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.7/dist-packages (from mbrl) (1.21.5)\n",
            "Collecting sk-video>=1.1.10\n",
            "  Downloading sk_video-1.1.10-py2.py3-none-any.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 62.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from mbrl) (1.1.0)\n",
            "Collecting matplotlib>=3.3.1\n",
            "  Downloading matplotlib-3.5.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 51.6 MB/s \n",
            "\u001b[?25hCollecting pytest>=6.0.1\n",
            "  Downloading pytest-7.1.1-py3-none-any.whl (297 kB)\n",
            "\u001b[K     |████████████████████████████████| 297 kB 59.8 MB/s \n",
            "\u001b[?25hCollecting hydra-core==1.0.3\n",
            "  Downloading hydra_core-1.0.3-py3-none-any.whl (122 kB)\n",
            "\u001b[K     |████████████████████████████████| 122 kB 71.2 MB/s \n",
            "\u001b[?25hCollecting gym==0.17.2\n",
            "  Downloading gym-0.17.2.tar.gz (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 53.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from mbrl) (2.8.0)\n",
            "Requirement already satisfied: jupyter>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from mbrl) (1.0.0)\n",
            "Collecting imageio>=2.9.0\n",
            "  Downloading imageio-2.16.2-py3-none-any.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 52.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym==0.17.2->mbrl) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.17.2->mbrl) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.17.2->mbrl) (1.3.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core==1.0.3->mbrl) (5.6.0)\n",
            "Collecting omegaconf>=2.0.2\n",
            "  Downloading omegaconf-2.1.2-py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 4.4 MB/s \n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 79.2 MB/s \n",
            "\u001b[?25hCollecting pillow>=8.3.2\n",
            "  Downloading Pillow-9.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 52.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0.0->mbrl) (4.10.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0.0->mbrl) (5.2.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0.0->mbrl) (5.6.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0.0->mbrl) (7.7.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0.0->mbrl) (5.3.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0.0->mbrl) (5.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.1->mbrl) (2.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.1->mbrl) (21.3)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.1->mbrl) (3.0.8)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.1->mbrl) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.1->mbrl) (1.4.2)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.32.0-py3-none-any.whl (900 kB)\n",
            "\u001b[K     |████████████████████████████████| 900 kB 52.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.3.1->mbrl) (4.1.1)\n",
            "Collecting PyYAML>=5.1.0\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 59.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.2->mbrl) (0.16.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=6.0.1->mbrl) (21.4.0)\n",
            "Collecting pluggy<2.0,>=0.12\n",
            "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=6.0.1->mbrl) (2.0.1)\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest>=6.0.1->mbrl) (1.11.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.7/dist-packages (from pytest>=6.0.1->mbrl) (1.1.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest>=6.0.1->mbrl) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest>=6.0.1->mbrl) (3.8.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.1->mbrl) (1.15.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.0->mbrl) (0.4.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.0->mbrl) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.0->mbrl) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.0->mbrl) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.0->mbrl) (1.44.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.0->mbrl) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.0->mbrl) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.0->mbrl) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.0->mbrl) (1.35.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.0->mbrl) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.0->mbrl) (1.0.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.0->mbrl) (57.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.0->mbrl) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.0->mbrl) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.0->mbrl) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.0->mbrl) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.0->mbrl) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.4.0->mbrl) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.4.0->mbrl) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.4.0->mbrl) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.4.0->mbrl) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.0->mbrl) (3.2.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter>=1.0.0->mbrl) (5.3.5)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter>=1.0.0->mbrl) (5.5.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter>=1.0.0->mbrl) (5.1.1)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter>=1.0.0->mbrl) (5.1.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter>=1.0.0->mbrl) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter>=1.0.0->mbrl) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter>=1.0.0->mbrl) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter>=1.0.0->mbrl) (0.8.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter>=1.0.0->mbrl) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter>=1.0.0->mbrl) (0.7.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->jupyter>=1.0.0->mbrl) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter>=1.0.0->mbrl) (0.2.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter>=1.0.0->mbrl) (5.3.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter>=1.0.0->mbrl) (3.6.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter>=1.0.0->mbrl) (1.1.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter>=1.0.0->mbrl) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter>=1.0.0->mbrl) (4.9.2)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter>=1.0.0->mbrl) (2.15.3)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter>=1.0.0->mbrl) (0.18.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0.0->mbrl) (2.11.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0.0->mbrl) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0.0->mbrl) (0.13.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter>=1.0.0->mbrl) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter>=1.0.0->mbrl) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter>=1.0.0->mbrl) (2.0.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0.0->mbrl) (0.6.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0.0->mbrl) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0.0->mbrl) (0.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0.0->mbrl) (5.0.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0.0->mbrl) (0.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0.0->mbrl) (1.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter>=1.0.0->mbrl) (0.5.1)\n",
            "Requirement already satisfied: qtpy>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter>=1.0.0->mbrl) (2.0.1)\n",
            "Building wheels for collected packages: gym, antlr4-python3-runtime\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.17.2-py3-none-any.whl size=1650890 sha256=8ad9918bbdd2703023fdf727e729f38a824b85c87cda2881f42d3c8257064f2a\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/e1/58/89a2aa24e6c2cc800204fc02010612afdf200926c4d6bfe315\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=55a229bd976c1f890a97c0024fd564f0778987bfd219b9606b43b6b59c4e8fad\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "Successfully built gym antlr4-python3-runtime\n",
            "Installing collected packages: PyYAML, antlr4-python3-runtime, pluggy, pillow, omegaconf, fonttools, sk-video, pytest, matplotlib, imageio, hydra-core, gym, mbrl\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: pluggy\n",
            "    Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Attempting uninstall: imageio\n",
            "    Found existing installation: imageio 2.4.1\n",
            "    Uninstalling imageio-2.4.1:\n",
            "      Successfully uninstalled imageio-2.4.1\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed PyYAML-6.0 antlr4-python3-runtime-4.8 fonttools-4.32.0 gym-0.17.2 hydra-core-1.0.3 imageio-2.16.2 matplotlib-3.5.1 mbrl-0.1.5 omegaconf-2.1.2 pillow-9.1.0 pluggy-1.0.0 pytest-7.1.1 sk-video-1.1.10\n"
          ]
        }
      ],
      "source": [
        "%matplotlib notebook\n",
        "!python -m pip install mpi4py\n",
        "!python -m pip install mbrl\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import random\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import mbrl.util.math\n",
        "from sklearn import preprocessing\n",
        "from mpi4py import MPI \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "comm = MPI.COMM_WORLD\n",
        "size = comm.Get_size()\n",
        "rank = comm.Get_rank()\n",
        "boss = rank==0\n",
        "# Seed RNG\n",
        "np.random.seed(rank)\n",
        "# python -m pip install mpi4py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLggIEFE7mMx",
        "outputId": "db5cecd1-8cdf-434b-cb6c-caaadc4df64f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/ziyan/ILQR/training data.csv')\n",
        "test_data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/ziyan/ILQR/test data.csv')\n",
        "valid_data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/ziyan/ILQR/valid data.csv')\n",
        "training_data = np.array(training_data)\n",
        "testt_data = np.array(test_data)\n",
        "test_data = np.array(test_data)\n",
        "valid_data = np.array(valid_data)"
      ],
      "metadata": {
        "id": "FJoVss_Z7oxm"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_data[0,:])\n",
        "scalerX = StandardScaler().fit(training_data[:,7:14])\n",
        "scalery = StandardScaler().fit(training_data[:,-7:])\n",
        "training_data[:,7:14] = scalerX.transform(training_data[:,7:14])\n",
        "training_data[:,-7:] = scalery.transform(training_data[:,-7:])\n",
        "test_data[:,7:14] = scalerX.transform(test_data[:,7:14])\n",
        "test_data[:,-7:] = scalery.transform(test_data[:,-7:])\n",
        "valid_data[:,7:14] = scalerX.transform(valid_data[:,7:14])\n",
        "valid_data[:,-7:] = scalery.transform(valid_data[:,-7:])\n",
        "print(training_data[0,:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcaR4rI6Y-fD",
        "outputId": "1961bc02-5e86-48df-9e56-443ab421aae7"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  1.3467995    1.0224129   -1.7369454    0.98558265   0.7130162\n",
            "  -0.70451635   0.04044396 -10.758503   -10.593262    -2.9916844\n",
            " -21.944834     6.845678     5.5541124  -14.982534    12.548387\n",
            " -39.394768    37.680687   -34.75261    -33.981106    33.326324\n",
            "  -0.75313205   1.2910267    0.9443954   -1.8109274    0.6838323\n",
            "   0.6794432   -0.41811532   0.09664377  -5.577281    -7.801746\n",
            "  -7.398209   -30.175035    -3.357305    28.6401       5.6199813 ]\n",
            "[  1.3467995    1.0224129   -1.7369454    0.98558265   0.7130162\n",
            "  -0.70451635   0.04044396  -0.63276274  -0.61573357  -0.18090254\n",
            "  -1.30396275   0.40756423   0.33634883  -0.83762617  12.548387\n",
            " -39.394768    37.680687   -34.75261    -33.981106    33.326324\n",
            "  -0.75313205   1.2910267    0.9443954   -1.8109274    0.6838323\n",
            "   0.6794432   -0.41811532   0.09664377  -0.28264525  -0.45192007\n",
            "  -0.37051383  -1.41322991  -0.11031186   0.91180135   0.27711511]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_data[0,:])\n",
        "scalerX = StandardScaler().fit(training_data[:,:21])\n",
        "scalery = StandardScaler().fit(training_data[:,-14:])\n",
        "training_data[:,:21] = scalerX.transform(training_data[:,:21])\n",
        "training_data[:,-14:] = scalery.transform(training_data[:,-14:])\n",
        "test_data[:,:21] = scalerX.transform(test_data[:,:21])\n",
        "test_data[:,-14:] = scalery.transform(test_data[:,-14:])\n",
        "valid_data[:,:21] = scalerX.transform(valid_data[:,:21])\n",
        "valid_data[:,-14:] = scalery.transform(valid_data[:,-14:])\n",
        "print(training_data[0,:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kq_hJlbTsSdr",
        "outputId": "de534bef-0403-492f-e61a-dece7fa0998b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  1.3467995    1.0224129   -1.7369454    0.98558265   0.7130162\n",
            "  -0.70451635   0.04044396 -10.758503   -10.593262    -2.9916844\n",
            " -21.944834     6.845678     5.5541124  -14.982534    12.548387\n",
            " -39.394768    37.680687   -34.75261    -33.981106    33.326324\n",
            "  -0.75313205   1.2910267    0.9443954   -1.8109274    0.6838323\n",
            "   0.6794432   -0.41811532   0.09664377  -5.577281    -7.801746\n",
            "  -7.398209   -30.175035    -3.357305    28.6401       5.6199813 ]\n",
            "[ 1.13583613  0.88480469 -1.52811344  0.845197    0.61552332 -0.60419242\n",
            "  0.04821724 -0.63276274 -0.61573357 -0.18090254 -1.30396275  0.40756423\n",
            "  0.33634883 -0.83762617  0.67264551 -1.33067774  1.63294522 -1.18465577\n",
            " -1.10318896  1.49558414  0.16841617  1.06998401  0.81092974 -1.56790541\n",
            "  0.61283708  0.58413752 -0.39615061  0.14918687 -0.28264525 -0.45192007\n",
            " -0.37051383 -1.41322991 -0.11031186  0.91180135  0.27711511]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#index = [7,8,9,10,11,12,13,34,33,32,31,30,29,28]\n",
        "index = [34,33,32,31,30,29,28]\n",
        "print(training_data[0,:])\n",
        "for i in range(3000):\n",
        "  for j in index:\n",
        "    training_data[i,j] = training_data[i,j] / 10\n",
        "for i in range(3000):\n",
        "  for j in index:\n",
        "    test_data[i,j] = test_data[i,j] / 10\n",
        "for i in range(3000):\n",
        "  for j in index:\n",
        "    valid_data[i,j] = valid_data[i,j] / 10\n",
        "print(training_data[0,:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INstRRUiJBNX",
        "outputId": "c9d025de-5b06-4194-af4f-ba32a4855e60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  1.3467995    1.0224129   -1.7369454    0.98558265   0.7130162\n",
            "  -0.70451635   0.04044396 -10.758503   -10.593262    -2.9916844\n",
            " -21.944834     6.845678     5.5541124  -14.982534    12.548387\n",
            " -39.394768    37.680687   -34.75261    -33.981106    33.326324\n",
            "  -0.75313205   1.2910267    0.9443954   -1.8109274    0.6838323\n",
            "   0.6794432   -0.41811532   0.09664377  -5.577281    -7.801746\n",
            "  -7.398209   -30.175035    -3.357305    28.6401       5.6199813 ]\n",
            "[  1.3467995    1.0224129   -1.7369454    0.98558265   0.7130162\n",
            "  -0.70451635   0.04044396 -10.758503   -10.593262    -2.9916844\n",
            " -21.944834     6.845678     5.5541124  -14.982534    12.548387\n",
            " -39.394768    37.680687   -34.75261    -33.981106    33.326324\n",
            "  -0.75313205   1.2910267    0.9443954   -1.8109274    0.6838323\n",
            "   0.6794432   -0.41811532   0.09664377  -0.5577281   -0.7801746\n",
            "  -0.7398209   -3.0175035   -0.3357305    2.86401      0.56199813]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, learning_rate, seed):\n",
        "\n",
        "    super(Model, self).__init__()\n",
        "    torch.manual_seed(seed)\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.hidden_units = 200\n",
        "\n",
        "    self.min_logvar = nn.Parameter(\n",
        "                -10 * torch.ones(1, self.output_dim//2), requires_grad=False\n",
        "            )\n",
        "    self.max_logvar = nn.Parameter(\n",
        "                0.5 * torch.ones(1, self.output_dim//2), requires_grad=False\n",
        "            )\n",
        "    #self.learn_logvar_bounds: bool = False\n",
        "\n",
        "    \n",
        "        # Instantiate model\n",
        "    self.model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(self.input_dim, self.hidden_units, bias=True),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(self.hidden_units, self.hidden_units, bias=True),\n",
        "    torch.nn.ReLU(),\n",
        "    #torch.nn.Linear(self.hidden_units, self.hidden_units, bias=True),\n",
        "    #torch.nn.ReLU(),\n",
        "    #torch.nn.Linear(self.hidden_units, self.hidden_units, bias=True),\n",
        "    #torch.nn.ReLU(),\n",
        "    torch.nn.Linear(self.hidden_units, self.hidden_units, bias=True),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(self.hidden_units, self.output_dim, bias=True)\n",
        "    ).to(torch.device('cpu'))\n",
        "    # Instantiate optimizer\n",
        "    self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "\n",
        "  def adjust_learning_rate(self):\n",
        "    for param_group in self.optimizer.param_groups:\n",
        "      param_group['lr'] = param_group['lr']*0.999\n",
        "\n",
        "  def softplus(self, x):\n",
        "    \"\"\" Compute softplus \"\"\"\n",
        "    softplus = torch.log(1+torch.exp(x))\n",
        "    # Avoid infinities due to taking the exponent\n",
        "    softplus = torch.where(softplus==float('inf'), x, softplus)\n",
        "    return softplus\n",
        "\n",
        "  def NLL(self, mean, var, truth):\n",
        "    #diff = torch.sub(truth, mean)\n",
        "    #var = self.softplus(var)\n",
        "      # Compute loss \n",
        "    #loss = torch.mean(torch.div(diff**2, 2*var))\n",
        "    #loss += torch.mean(0.5*torch.log(var))\n",
        "    loss = (mbrl.util.math.gaussian_nll(mean, var, truth, reduce=False)\n",
        "         .mean(axis=0)  # average over batch and target dimension\n",
        "         .sum())\n",
        "         # sum over ensemble dimension\n",
        "    loss += 0.01 * (self.max_logvar.sum() - self.min_logvar.sum())\n",
        "    return loss\n",
        "\n",
        "  def forward(self, inputs, input_type):\n",
        "    \"\"\" Forward pass for a given input\n",
        "        :input:     (state,action)-pair\n",
        "        :returns:   means and variances given input\n",
        "    \"\"\"\n",
        "    # Compute output of model\n",
        "    if input_type == \"nparray\":            \n",
        "      x = torch.from_numpy(inputs).float()\n",
        "      x = x.view(-1,self.input_dim)\n",
        "      x = x.cuda()\n",
        "      out = self.model(x)\n",
        "      mean, var = torch.split(out, self.output_dim//2, dim=1)\n",
        "      #var = self.softplus(var)\n",
        "      var = self.max_logvar - F.softplus(self.max_logvar - var)\n",
        "      var = self.min_logvar + F.softplus(var - self.min_logvar)\n",
        "      #mean.detach().numpy(), var.detach().numpy()\n",
        "      return mean, var\n",
        "    else:\n",
        "      inputs = inputs.view(-1,self.input_dim)\n",
        "      inputs = inputs.cuda()\n",
        "      out = self.model(inputs)\n",
        "      mean, var = torch.split(out, self.output_dim//2, dim=1)\n",
        "      var = self.max_logvar - F.softplus(self.max_logvar - var)\n",
        "      var = self.min_logvar + F.softplus(var - self.min_logvar)\n",
        "      return mean, var\n",
        "      \n",
        "  def step(self, inputs, true_out):\n",
        "    \"\"\" Execute gradient step given the samples in the minibatch \"\"\"\n",
        "    # Convert input and true_out to useable tensors\n",
        "    #x = torch.from_numpy(inputs).float()\n",
        "    #y = torch.from_numpy(true_out).float()\n",
        "    x = inputs\n",
        "    y = true_out  \n",
        "\n",
        "    # Compute output of model\n",
        "    out = self.model(x)\n",
        "    mean, var = torch.split(out, self.output_dim//2, dim=1)\n",
        "    var = self.max_logvar - F.softplus(self.max_logvar - var)\n",
        "    var = self.min_logvar + F.softplus(var - self.min_logvar)\n",
        "    \n",
        "\n",
        "\n",
        "    # Compute loss \n",
        "    self.nll = self.NLL(mean, var, y)\n",
        "\n",
        "    # Backpropagate the loss\n",
        "    self.optimizer.zero_grad()\n",
        "    self.nll.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 10)\n",
        "    self.optimizer.step()\n",
        "\n",
        "    self.adjust_learning_rate()\n",
        "\n",
        "  def compute_errors(self, train_in, validation_in, test_in):\n",
        "    \"\"\" Compute loss on the training, validation and test data \"\"\"\n",
        "    # Training data\n",
        "    train_in = torch.from_numpy(train_in).float()\n",
        "    train_in = train_in.cuda()\n",
        "    train_in, train_out = torch.split(train_in, [21,14], dim=1)\n",
        "    mean, var = self.forward(train_in, \"tensor\")\n",
        "    train_loss = self.NLL(mean, var, train_out).item()\n",
        "\n",
        "    # Validation data\n",
        "    validation_in = torch.from_numpy(validation_in).float()\n",
        "    validation_in = validation_in.cuda()\n",
        "    validation_in, val_out = torch.split(validation_in, [21,14], dim=1)\n",
        "    mean, var = self.forward(validation_in, \"tensor\")\n",
        "    val_loss = self.NLL(mean, var, val_out).item()\n",
        "\n",
        "    # Test data\n",
        "    test_in = torch.from_numpy(test_in).float()\n",
        "    test_in = test_in.cuda()\n",
        "    test_in, test_out = torch.split(test_in, [21,14], dim=1)\n",
        "    mean, var = self.forward(test_in, \"tensor\")\n",
        "    test_loss = self.NLL(mean, var, test_out).item()\n",
        "\n",
        "    return train_loss, val_loss, test_loss"
      ],
      "metadata": {
        "id": "GrwpZQiC7rYp"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_size = 4          # Ensemble size per core\n",
        "epochs = 15000\n",
        "learning_rate = 0.0001\n",
        "training_samples = 3000\n",
        "validation_samples = training_samples\n",
        "test_samples = 3000\n",
        "batch_size = 128\n",
        "measurements = epochs//200  # Measure every n steps\n",
        "\n",
        "# Define network architecture\n",
        "input_dim = 21\n",
        "output_dim = 28\n",
        "    # Define systems\n",
        "    # current available data sets: SimpleTwoDimensional(N, seed)\n",
        "datamic_systems = [\"linear\"]\n",
        "\n",
        "seeds = np.random.randint(1e4, size=ensemble_size)\n",
        "    # Allocate for losses\n",
        "train_error = np.zeros((ensemble_size, measurements))\n",
        "validation_error = np.zeros(train_error.shape)\n",
        "test_error = np.zeros(train_error.shape)\n",
        "    \n",
        "for system in datamic_systems:\n",
        "        # Instantiate class objects\n",
        "        # data = datamics.datamics(training_samples, system)\n",
        "        # Initialize models\n",
        "  ensemble = [Model(input_dim, output_dim, learning_rate, seeds[i]) for i in range(ensemble_size)]\n",
        "      # Initialize & allocate\n",
        "  ensemble_mean = np.zeros((test_samples, 14))\n",
        "  ensemble_var = np.zeros((test_samples, 14))\n",
        "\n",
        "      \n",
        "\n",
        "  # Train an ensemble of probabilistic networks:\n",
        "  i = 0\n",
        "  for model in ensemble:\n",
        "    model = model.cuda()\n",
        "    j = 0\n",
        "    for epoch in range(epochs):\n",
        "      indices = np.random.choice(range(training_samples-1), size=batch_size,replace=False)\n",
        "      minibatch=training_data[indices]\n",
        "      model_in = minibatch[:,:21]\n",
        "      y = minibatch[:,21:]\n",
        "      model_in = torch.from_numpy(model_in)\n",
        "      y = torch.from_numpy(y).float()\n",
        "      model_in = model_in.cuda().float()\n",
        "      y = y.cuda()\n",
        "      model.step(model_in, y)\n",
        "\n",
        "      #scheduler.step()\n",
        "      if (epoch+1)%(epochs//measurements)==0:\n",
        "        # Compute error on train, test and validation sets\n",
        "        train_error[i,j], validation_error[i,j], test_error[i,j] = model.compute_errors(training_data, valid_data, test_data)\n",
        "        j += 1\n",
        "      #if (epoch+1)%100 == 0:\n",
        "        print('Epoch %s, Train loss %s, Valid loss %s, Test loss %s'%(epoch, train_error[i,j-1],validation_error[i,j-1], test_error[i,j-1]))\n",
        "\n",
        "      # Test model on training samples\n",
        "\n",
        "  #mean, var = model.forward(test_data[:,:21], \"nparray\")\n",
        "\n",
        "\n",
        "      # Add to ensemble mean and variance\n",
        "  #ensemble_mean += mean \n",
        "  #ensemble_var += var + mean**2\n",
        "\n",
        "  i += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7yu4ktrDwJl",
        "outputId": "4374f0e9-d07d-4af4-ef22-7979ffb048c2"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 199, Train loss 14.207433700561523, Valid loss 15.43763256072998, Test loss 15.226951599121094\n",
            "Epoch 399, Train loss 13.387285232543945, Valid loss 15.251978874206543, Test loss 14.989690780639648\n",
            "Epoch 599, Train loss 12.690532684326172, Valid loss 15.254193305969238, Test loss 14.932465553283691\n",
            "Epoch 799, Train loss 12.02305793762207, Valid loss 15.024203300476074, Test loss 14.777741432189941\n",
            "Epoch 999, Train loss 11.3355712890625, Valid loss 14.515841484069824, Test loss 14.218672752380371\n",
            "Epoch 1199, Train loss 10.687066078186035, Valid loss 14.370747566223145, Test loss 14.049384117126465\n",
            "Epoch 1399, Train loss 10.066839218139648, Valid loss 13.978588104248047, Test loss 13.598421096801758\n",
            "Epoch 1599, Train loss 9.50196361541748, Valid loss 13.667015075683594, Test loss 13.271349906921387\n",
            "Epoch 1799, Train loss 9.002838134765625, Valid loss 13.528419494628906, Test loss 13.123940467834473\n",
            "Epoch 1999, Train loss 8.589351654052734, Valid loss 13.023871421813965, Test loss 12.68505573272705\n",
            "Epoch 2199, Train loss 8.234671592712402, Valid loss 13.010807991027832, Test loss 12.605767250061035\n",
            "Epoch 2399, Train loss 7.94116735458374, Valid loss 12.727004051208496, Test loss 12.343159675598145\n",
            "Epoch 2599, Train loss 7.693657398223877, Valid loss 12.530288696289062, Test loss 12.174783706665039\n",
            "Epoch 2799, Train loss 7.497556686401367, Valid loss 12.424635887145996, Test loss 12.053621292114258\n",
            "Epoch 2999, Train loss 7.321424961090088, Valid loss 12.2944917678833, Test loss 11.92598819732666\n",
            "Epoch 3199, Train loss 7.1887078285217285, Valid loss 12.168681144714355, Test loss 11.794442176818848\n",
            "Epoch 3399, Train loss 7.073382377624512, Valid loss 12.14206314086914, Test loss 11.766549110412598\n",
            "Epoch 3599, Train loss 6.980981349945068, Valid loss 12.060641288757324, Test loss 11.681483268737793\n",
            "Epoch 3799, Train loss 6.904228687286377, Valid loss 12.010157585144043, Test loss 11.652548789978027\n",
            "Epoch 3999, Train loss 6.8422770500183105, Valid loss 12.050087928771973, Test loss 11.675825119018555\n",
            "Epoch 4199, Train loss 6.791576862335205, Valid loss 11.995421409606934, Test loss 11.619601249694824\n",
            "Epoch 4399, Train loss 6.750589370727539, Valid loss 11.938586235046387, Test loss 11.564180374145508\n",
            "Epoch 4599, Train loss 6.717092990875244, Valid loss 11.8663330078125, Test loss 11.498351097106934\n",
            "Epoch 4799, Train loss 6.688340663909912, Valid loss 11.909082412719727, Test loss 11.528717994689941\n",
            "Epoch 4999, Train loss 6.665246486663818, Valid loss 11.888089179992676, Test loss 11.508429527282715\n",
            "Epoch 5199, Train loss 6.647076606750488, Valid loss 11.878746032714844, Test loss 11.50733470916748\n",
            "Epoch 5399, Train loss 6.631404399871826, Valid loss 11.861889839172363, Test loss 11.48902416229248\n",
            "Epoch 5599, Train loss 6.618968963623047, Valid loss 11.859640121459961, Test loss 11.478361129760742\n",
            "Epoch 5799, Train loss 6.608687400817871, Valid loss 11.856130599975586, Test loss 11.475285530090332\n",
            "Epoch 5999, Train loss 6.6004486083984375, Valid loss 11.840056419372559, Test loss 11.462126731872559\n",
            "Epoch 6199, Train loss 6.593469619750977, Valid loss 11.836607933044434, Test loss 11.459394454956055\n",
            "Epoch 6399, Train loss 6.587975978851318, Valid loss 11.826751708984375, Test loss 11.450236320495605\n",
            "Epoch 6599, Train loss 6.5833353996276855, Valid loss 11.827359199523926, Test loss 11.450722694396973\n",
            "Epoch 6799, Train loss 6.579577922821045, Valid loss 11.828014373779297, Test loss 11.45133113861084\n",
            "Epoch 6999, Train loss 6.576530456542969, Valid loss 11.830218315124512, Test loss 11.452567100524902\n",
            "Epoch 7199, Train loss 6.574029922485352, Valid loss 11.826884269714355, Test loss 11.449225425720215\n",
            "Epoch 7399, Train loss 6.571994304656982, Valid loss 11.822093963623047, Test loss 11.445536613464355\n",
            "Epoch 7599, Train loss 6.570374011993408, Valid loss 11.821410179138184, Test loss 11.444149017333984\n",
            "Epoch 7799, Train loss 6.56899881362915, Valid loss 11.821396827697754, Test loss 11.44368839263916\n",
            "Epoch 7999, Train loss 6.567832946777344, Valid loss 11.81921672821045, Test loss 11.442458152770996\n",
            "Epoch 8199, Train loss 6.566939830780029, Valid loss 11.820262908935547, Test loss 11.443215370178223\n",
            "Epoch 8399, Train loss 6.566169738769531, Valid loss 11.821154594421387, Test loss 11.4436674118042\n",
            "Epoch 8599, Train loss 6.565541744232178, Valid loss 11.822046279907227, Test loss 11.44457721710205\n",
            "Epoch 8799, Train loss 6.565043926239014, Valid loss 11.821166038513184, Test loss 11.444010734558105\n",
            "Epoch 8999, Train loss 6.56464147567749, Valid loss 11.82030963897705, Test loss 11.443193435668945\n",
            "Epoch 9199, Train loss 6.564326763153076, Valid loss 11.820536613464355, Test loss 11.443267822265625\n",
            "Epoch 9399, Train loss 6.564072608947754, Valid loss 11.82058048248291, Test loss 11.443121910095215\n",
            "Epoch 9599, Train loss 6.56389045715332, Valid loss 11.820229530334473, Test loss 11.442900657653809\n",
            "Epoch 9799, Train loss 6.563752174377441, Valid loss 11.82013988494873, Test loss 11.442931175231934\n",
            "Epoch 9999, Train loss 6.563660144805908, Valid loss 11.82011604309082, Test loss 11.442922592163086\n",
            "Epoch 10199, Train loss 6.563595294952393, Valid loss 11.819965362548828, Test loss 11.442808151245117\n",
            "Epoch 10399, Train loss 6.563554763793945, Valid loss 11.819982528686523, Test loss 11.442788124084473\n",
            "Epoch 10599, Train loss 6.563525676727295, Valid loss 11.819989204406738, Test loss 11.442817687988281\n",
            "Epoch 10799, Train loss 6.56350564956665, Valid loss 11.819973945617676, Test loss 11.442811965942383\n",
            "Epoch 10999, Train loss 6.563492774963379, Valid loss 11.81994342803955, Test loss 11.442792892456055\n",
            "Epoch 11199, Train loss 6.563485622406006, Valid loss 11.819939613342285, Test loss 11.442791938781738\n",
            "Epoch 11399, Train loss 6.563479423522949, Valid loss 11.819930076599121, Test loss 11.442781448364258\n",
            "Epoch 11599, Train loss 6.563476085662842, Valid loss 11.819927215576172, Test loss 11.442777633666992\n",
            "Epoch 11799, Train loss 6.563473701477051, Valid loss 11.81992244720459, Test loss 11.442774772644043\n",
            "Epoch 11999, Train loss 6.56347131729126, Valid loss 11.819914817810059, Test loss 11.442769050598145\n",
            "Epoch 12199, Train loss 6.563470840454102, Valid loss 11.819914817810059, Test loss 11.442767143249512\n",
            "Epoch 12399, Train loss 6.563469886779785, Valid loss 11.819910049438477, Test loss 11.442765235900879\n",
            "Epoch 12599, Train loss 6.563469409942627, Valid loss 11.81990909576416, Test loss 11.442765235900879\n",
            "Epoch 12799, Train loss 6.5634684562683105, Valid loss 11.81990909576416, Test loss 11.442765235900879\n",
            "Epoch 12999, Train loss 6.5634684562683105, Valid loss 11.819908142089844, Test loss 11.442764282226562\n",
            "Epoch 13199, Train loss 6.5634684562683105, Valid loss 11.819907188415527, Test loss 11.442764282226562\n",
            "Epoch 13399, Train loss 6.5634684562683105, Valid loss 11.819907188415527, Test loss 11.442764282226562\n",
            "Epoch 13599, Train loss 6.5634684562683105, Valid loss 11.819907188415527, Test loss 11.442764282226562\n",
            "Epoch 13799, Train loss 6.5634684562683105, Valid loss 11.819907188415527, Test loss 11.442763328552246\n",
            "Epoch 13999, Train loss 6.5634684562683105, Valid loss 11.819907188415527, Test loss 11.442763328552246\n",
            "Epoch 14199, Train loss 6.5634684562683105, Valid loss 11.819907188415527, Test loss 11.442764282226562\n",
            "Epoch 14399, Train loss 6.5634684562683105, Valid loss 11.819907188415527, Test loss 11.442764282226562\n",
            "Epoch 14599, Train loss 6.5634684562683105, Valid loss 11.819907188415527, Test loss 11.442763328552246\n",
            "Epoch 14799, Train loss 6.5634684562683105, Valid loss 11.819907188415527, Test loss 11.442764282226562\n",
            "Epoch 14999, Train loss 6.5634684562683105, Valid loss 11.819907188415527, Test loss 11.442764282226562\n",
            "Epoch 199, Train loss 14.319262504577637, Valid loss 15.588066101074219, Test loss 15.451940536499023\n",
            "Epoch 399, Train loss 13.502141952514648, Valid loss 15.392913818359375, Test loss 15.258064270019531\n",
            "Epoch 599, Train loss 12.87552547454834, Valid loss 15.36038875579834, Test loss 15.202916145324707\n",
            "Epoch 799, Train loss 12.285344123840332, Valid loss 15.225251197814941, Test loss 15.130118370056152\n",
            "Epoch 999, Train loss 11.723130226135254, Valid loss 14.957806587219238, Test loss 14.85722541809082\n",
            "Epoch 1199, Train loss 11.2010498046875, Valid loss 14.708269119262695, Test loss 14.60432243347168\n",
            "Epoch 1399, Train loss 10.656241416931152, Valid loss 14.615114212036133, Test loss 14.556063652038574\n",
            "Epoch 1599, Train loss 10.187898635864258, Valid loss 14.397543907165527, Test loss 14.3065185546875\n",
            "Epoch 1799, Train loss 9.74223804473877, Valid loss 14.200125694274902, Test loss 14.106860160827637\n",
            "Epoch 1999, Train loss 9.365324020385742, Valid loss 13.933812141418457, Test loss 13.834528923034668\n",
            "Epoch 2199, Train loss 9.024467468261719, Valid loss 13.675134658813477, Test loss 13.561018943786621\n",
            "Epoch 2399, Train loss 8.722413063049316, Valid loss 13.72765827178955, Test loss 13.65039348602295\n",
            "Epoch 2599, Train loss 8.489015579223633, Valid loss 13.45785140991211, Test loss 13.373286247253418\n",
            "Epoch 2799, Train loss 8.281457901000977, Valid loss 13.449803352355957, Test loss 13.330950736999512\n",
            "Epoch 2999, Train loss 8.120835304260254, Valid loss 13.246976852416992, Test loss 13.151610374450684\n",
            "Epoch 3199, Train loss 7.968931674957275, Valid loss 13.145411491394043, Test loss 13.058691024780273\n",
            "Epoch 3399, Train loss 7.849194526672363, Valid loss 13.006535530090332, Test loss 12.879688262939453\n",
            "Epoch 3599, Train loss 7.754001617431641, Valid loss 12.99848747253418, Test loss 12.861547470092773\n",
            "Epoch 3799, Train loss 7.673276901245117, Valid loss 12.99112606048584, Test loss 12.85809326171875\n",
            "Epoch 3999, Train loss 7.6089653968811035, Valid loss 12.885918617248535, Test loss 12.749028205871582\n",
            "Epoch 4199, Train loss 7.554328918457031, Valid loss 12.866947174072266, Test loss 12.721671104431152\n",
            "Epoch 4399, Train loss 7.51024055480957, Valid loss 12.890219688415527, Test loss 12.739315032958984\n",
            "Epoch 4599, Train loss 7.474043369293213, Valid loss 12.842452049255371, Test loss 12.689743041992188\n",
            "Epoch 4799, Train loss 7.444741725921631, Valid loss 12.835850715637207, Test loss 12.678366661071777\n",
            "Epoch 4999, Train loss 7.420560359954834, Valid loss 12.833627700805664, Test loss 12.675843238830566\n",
            "Epoch 5199, Train loss 7.400314807891846, Valid loss 12.822356224060059, Test loss 12.672575950622559\n",
            "Epoch 5399, Train loss 7.384555816650391, Valid loss 12.809916496276855, Test loss 12.658724784851074\n",
            "Epoch 5599, Train loss 7.371638774871826, Valid loss 12.811783790588379, Test loss 12.662379264831543\n",
            "Epoch 5799, Train loss 7.3603386878967285, Valid loss 12.788086891174316, Test loss 12.63756275177002\n",
            "Epoch 5999, Train loss 7.351473331451416, Valid loss 12.778254508972168, Test loss 12.628270149230957\n",
            "Epoch 6199, Train loss 7.34433126449585, Valid loss 12.763181686401367, Test loss 12.61267375946045\n",
            "Epoch 6399, Train loss 7.338294506072998, Valid loss 12.751978874206543, Test loss 12.600188255310059\n",
            "Epoch 6599, Train loss 7.3336262702941895, Valid loss 12.754572868347168, Test loss 12.605263710021973\n",
            "Epoch 6799, Train loss 7.32938814163208, Valid loss 12.761858940124512, Test loss 12.609396934509277\n",
            "Epoch 6999, Train loss 7.326078414916992, Valid loss 12.756447792053223, Test loss 12.60419750213623\n",
            "Epoch 7199, Train loss 7.323432445526123, Valid loss 12.751591682434082, Test loss 12.598786354064941\n",
            "Epoch 7399, Train loss 7.321291446685791, Valid loss 12.74794864654541, Test loss 12.595354080200195\n",
            "Epoch 7599, Train loss 7.319487571716309, Valid loss 12.74697494506836, Test loss 12.593086242675781\n",
            "Epoch 7799, Train loss 7.3180389404296875, Valid loss 12.746430397033691, Test loss 12.592484474182129\n",
            "Epoch 7999, Train loss 7.316829681396484, Valid loss 12.743971824645996, Test loss 12.589993476867676\n",
            "Epoch 8199, Train loss 7.315849781036377, Valid loss 12.744500160217285, Test loss 12.590498924255371\n",
            "Epoch 8399, Train loss 7.315026760101318, Valid loss 12.743343353271484, Test loss 12.589320182800293\n",
            "Epoch 8599, Train loss 7.314372539520264, Valid loss 12.743144989013672, Test loss 12.588995933532715\n",
            "Epoch 8799, Train loss 7.313836097717285, Valid loss 12.743034362792969, Test loss 12.589085578918457\n",
            "Epoch 8999, Train loss 7.313409805297852, Valid loss 12.74231243133545, Test loss 12.588458061218262\n",
            "Epoch 9199, Train loss 7.3130669593811035, Valid loss 12.742127418518066, Test loss 12.58840274810791\n",
            "Epoch 9399, Train loss 7.312799453735352, Valid loss 12.742003440856934, Test loss 12.588336944580078\n",
            "Epoch 9599, Train loss 7.312605857849121, Valid loss 12.742215156555176, Test loss 12.58859634399414\n",
            "Epoch 9799, Train loss 7.312466621398926, Valid loss 12.741914749145508, Test loss 12.588404655456543\n",
            "Epoch 9999, Train loss 7.312364101409912, Valid loss 12.74191665649414, Test loss 12.58835506439209\n",
            "Epoch 10199, Train loss 7.312293529510498, Valid loss 12.7418794631958, Test loss 12.588343620300293\n",
            "Epoch 10399, Train loss 7.3122477531433105, Valid loss 12.74185848236084, Test loss 12.588335990905762\n",
            "Epoch 10599, Train loss 7.3122172355651855, Valid loss 12.741830825805664, Test loss 12.588312149047852\n",
            "Epoch 10799, Train loss 7.312196731567383, Valid loss 12.741829872131348, Test loss 12.588318824768066\n",
            "Epoch 10999, Train loss 7.312182426452637, Valid loss 12.741798400878906, Test loss 12.588301658630371\n",
            "Epoch 11199, Train loss 7.3121724128723145, Valid loss 12.741798400878906, Test loss 12.588301658630371\n",
            "Epoch 11399, Train loss 7.312166213989258, Valid loss 12.741776466369629, Test loss 12.58828353881836\n",
            "Epoch 11599, Train loss 7.312161922454834, Valid loss 12.741759300231934, Test loss 12.588272094726562\n",
            "Epoch 11799, Train loss 7.312159061431885, Valid loss 12.74176025390625, Test loss 12.588275909423828\n",
            "Epoch 11999, Train loss 7.312157154083252, Valid loss 12.741759300231934, Test loss 12.588274955749512\n",
            "Epoch 12199, Train loss 7.312156677246094, Valid loss 12.7417573928833, Test loss 12.588274955749512\n",
            "Epoch 12399, Train loss 7.312155246734619, Valid loss 12.7417573928833, Test loss 12.588274955749512\n",
            "Epoch 12599, Train loss 7.312154769897461, Valid loss 12.741756439208984, Test loss 12.588274955749512\n",
            "Epoch 12799, Train loss 7.312154769897461, Valid loss 12.741756439208984, Test loss 12.588274955749512\n",
            "Epoch 12999, Train loss 7.312154293060303, Valid loss 12.741756439208984, Test loss 12.588274955749512\n",
            "Epoch 13199, Train loss 7.312154293060303, Valid loss 12.7417573928833, Test loss 12.588274955749512\n",
            "Epoch 13399, Train loss 7.312154293060303, Valid loss 12.7417573928833, Test loss 12.588274955749512\n",
            "Epoch 13599, Train loss 7.3121538162231445, Valid loss 12.741755485534668, Test loss 12.588274002075195\n",
            "Epoch 13799, Train loss 7.3121538162231445, Valid loss 12.7417573928833, Test loss 12.588274955749512\n",
            "Epoch 13999, Train loss 7.312153339385986, Valid loss 12.741756439208984, Test loss 12.588274002075195\n",
            "Epoch 14199, Train loss 7.312154293060303, Valid loss 12.7417573928833, Test loss 12.588274002075195\n",
            "Epoch 14399, Train loss 7.3121538162231445, Valid loss 12.7417573928833, Test loss 12.588274002075195\n",
            "Epoch 14599, Train loss 7.3121538162231445, Valid loss 12.7417573928833, Test loss 12.588274002075195\n",
            "Epoch 14799, Train loss 7.312153339385986, Valid loss 12.7417573928833, Test loss 12.588274002075195\n",
            "Epoch 14999, Train loss 7.3121538162231445, Valid loss 12.7417573928833, Test loss 12.588274955749512\n",
            "Epoch 199, Train loss 14.271842002868652, Valid loss 15.489184379577637, Test loss 15.283950805664062\n",
            "Epoch 399, Train loss 13.428936958312988, Valid loss 15.365821838378906, Test loss 15.203009605407715\n",
            "Epoch 599, Train loss 12.601130485534668, Valid loss 15.084811210632324, Test loss 14.943686485290527\n",
            "Epoch 799, Train loss 11.906706809997559, Valid loss 14.733121871948242, Test loss 14.557605743408203\n",
            "Epoch 999, Train loss 11.20551872253418, Valid loss 14.51079273223877, Test loss 14.296897888183594\n",
            "Epoch 1199, Train loss 10.484396934509277, Valid loss 14.008818626403809, Test loss 13.82265567779541\n",
            "Epoch 1399, Train loss 9.824337005615234, Valid loss 13.591272354125977, Test loss 13.423007011413574\n",
            "Epoch 1599, Train loss 9.248778343200684, Valid loss 13.286709785461426, Test loss 13.117903709411621\n",
            "Epoch 1799, Train loss 8.733729362487793, Valid loss 12.958268165588379, Test loss 12.757061004638672\n",
            "Epoch 1999, Train loss 8.299786567687988, Valid loss 12.627365112304688, Test loss 12.44965934753418\n",
            "Epoch 2199, Train loss 7.952419281005859, Valid loss 12.576821327209473, Test loss 12.399693489074707\n",
            "Epoch 2399, Train loss 7.657349109649658, Valid loss 12.280450820922852, Test loss 12.128561019897461\n",
            "Epoch 2599, Train loss 7.420399188995361, Valid loss 12.042122840881348, Test loss 11.886408805847168\n",
            "Epoch 2799, Train loss 7.2183756828308105, Valid loss 11.904215812683105, Test loss 11.752850532531738\n",
            "Epoch 2999, Train loss 7.062989234924316, Valid loss 11.912613868713379, Test loss 11.74258804321289\n",
            "Epoch 3199, Train loss 6.9310688972473145, Valid loss 11.801507949829102, Test loss 11.617327690124512\n",
            "Epoch 3399, Train loss 6.826269626617432, Valid loss 11.701696395874023, Test loss 11.535906791687012\n",
            "Epoch 3599, Train loss 6.738131999969482, Valid loss 11.689894676208496, Test loss 11.529597282409668\n",
            "Epoch 3799, Train loss 6.666552543640137, Valid loss 11.588857650756836, Test loss 11.420519828796387\n",
            "Epoch 3999, Train loss 6.607489109039307, Valid loss 11.586606979370117, Test loss 11.429895401000977\n",
            "Epoch 4199, Train loss 6.558831214904785, Valid loss 11.523449897766113, Test loss 11.347000122070312\n",
            "Epoch 4399, Train loss 6.521308422088623, Valid loss 11.530150413513184, Test loss 11.34350299835205\n",
            "Epoch 4599, Train loss 6.4875359535217285, Valid loss 11.469654083251953, Test loss 11.308392524719238\n",
            "Epoch 4799, Train loss 6.461131572723389, Valid loss 11.462026596069336, Test loss 11.291881561279297\n",
            "Epoch 4999, Train loss 6.440108299255371, Valid loss 11.463013648986816, Test loss 11.295132637023926\n",
            "Epoch 5199, Train loss 6.422776699066162, Valid loss 11.422524452209473, Test loss 11.251784324645996\n",
            "Epoch 5399, Train loss 6.4083781242370605, Valid loss 11.430702209472656, Test loss 11.257784843444824\n",
            "Epoch 5599, Train loss 6.396669387817383, Valid loss 11.421420097351074, Test loss 11.254016876220703\n",
            "Epoch 5799, Train loss 6.387159824371338, Valid loss 11.41060733795166, Test loss 11.23922061920166\n",
            "Epoch 5999, Train loss 6.3793535232543945, Valid loss 11.406052589416504, Test loss 11.232062339782715\n",
            "Epoch 6199, Train loss 6.37311315536499, Valid loss 11.404507637023926, Test loss 11.23270320892334\n",
            "Epoch 6399, Train loss 6.367850303649902, Valid loss 11.40247631072998, Test loss 11.229696273803711\n",
            "Epoch 6599, Train loss 6.363487720489502, Valid loss 11.397849082946777, Test loss 11.226107597351074\n",
            "Epoch 6799, Train loss 6.359957218170166, Valid loss 11.397319793701172, Test loss 11.225104331970215\n",
            "Epoch 6999, Train loss 6.357112407684326, Valid loss 11.399703025817871, Test loss 11.227824211120605\n",
            "Epoch 7199, Train loss 6.354843616485596, Valid loss 11.399885177612305, Test loss 11.228877067565918\n",
            "Epoch 7399, Train loss 6.3529205322265625, Valid loss 11.399889945983887, Test loss 11.228124618530273\n",
            "Epoch 7599, Train loss 6.351321220397949, Valid loss 11.400052070617676, Test loss 11.228593826293945\n",
            "Epoch 7799, Train loss 6.350079536437988, Valid loss 11.397759437561035, Test loss 11.226629257202148\n",
            "Epoch 7999, Train loss 6.349031925201416, Valid loss 11.396624565124512, Test loss 11.225359916687012\n",
            "Epoch 8199, Train loss 6.3481574058532715, Valid loss 11.3965425491333, Test loss 11.225123405456543\n",
            "Epoch 8399, Train loss 6.347435474395752, Valid loss 11.396088600158691, Test loss 11.224705696105957\n",
            "Epoch 8599, Train loss 6.3468451499938965, Valid loss 11.395539283752441, Test loss 11.224184036254883\n",
            "Epoch 8799, Train loss 6.346367835998535, Valid loss 11.39499568939209, Test loss 11.223429679870605\n",
            "Epoch 8999, Train loss 6.346001148223877, Valid loss 11.395146369934082, Test loss 11.223535537719727\n",
            "Epoch 9199, Train loss 6.345709323883057, Valid loss 11.395055770874023, Test loss 11.223620414733887\n",
            "Epoch 9399, Train loss 6.3454813957214355, Valid loss 11.394944190979004, Test loss 11.223560333251953\n",
            "Epoch 9599, Train loss 6.345314025878906, Valid loss 11.394598007202148, Test loss 11.223371505737305\n",
            "Epoch 9799, Train loss 6.345199108123779, Valid loss 11.394572257995605, Test loss 11.223356246948242\n",
            "Epoch 9999, Train loss 6.345113277435303, Valid loss 11.394673347473145, Test loss 11.22340202331543\n",
            "Epoch 10199, Train loss 6.345054626464844, Valid loss 11.39461612701416, Test loss 11.223343849182129\n",
            "Epoch 10399, Train loss 6.345015525817871, Valid loss 11.394638061523438, Test loss 11.223363876342773\n",
            "Epoch 10599, Train loss 6.344991207122803, Valid loss 11.39454174041748, Test loss 11.223273277282715\n",
            "Epoch 10799, Train loss 6.344973564147949, Valid loss 11.39450454711914, Test loss 11.223248481750488\n",
            "Epoch 10999, Train loss 6.344962120056152, Valid loss 11.39448356628418, Test loss 11.223219871520996\n",
            "Epoch 11199, Train loss 6.344954967498779, Valid loss 11.39448070526123, Test loss 11.223214149475098\n",
            "Epoch 11399, Train loss 6.344949722290039, Valid loss 11.394469261169434, Test loss 11.223198890686035\n",
            "Epoch 11599, Train loss 6.344946384429932, Valid loss 11.394461631774902, Test loss 11.223193168640137\n",
            "Epoch 11799, Train loss 6.344944477081299, Valid loss 11.394458770751953, Test loss 11.223193168640137\n",
            "Epoch 11999, Train loss 6.344942569732666, Valid loss 11.394455909729004, Test loss 11.223188400268555\n",
            "Epoch 12199, Train loss 6.344940662384033, Valid loss 11.394455909729004, Test loss 11.223187446594238\n",
            "Epoch 12399, Train loss 6.344940662384033, Valid loss 11.394454002380371, Test loss 11.223186492919922\n",
            "Epoch 12599, Train loss 6.344940185546875, Valid loss 11.394454002380371, Test loss 11.223186492919922\n",
            "Epoch 12799, Train loss 6.344940662384033, Valid loss 11.394454002380371, Test loss 11.223186492919922\n",
            "Epoch 12999, Train loss 6.344939708709717, Valid loss 11.394454002380371, Test loss 11.223187446594238\n",
            "Epoch 13199, Train loss 6.344939231872559, Valid loss 11.394454956054688, Test loss 11.223187446594238\n",
            "Epoch 13399, Train loss 6.344940185546875, Valid loss 11.394454002380371, Test loss 11.223187446594238\n",
            "Epoch 13599, Train loss 6.344939231872559, Valid loss 11.394454002380371, Test loss 11.223186492919922\n",
            "Epoch 13799, Train loss 6.344939708709717, Valid loss 11.394454002380371, Test loss 11.223187446594238\n",
            "Epoch 13999, Train loss 6.344939708709717, Valid loss 11.394454002380371, Test loss 11.223187446594238\n",
            "Epoch 14199, Train loss 6.344939708709717, Valid loss 11.394454002380371, Test loss 11.223187446594238\n",
            "Epoch 14399, Train loss 6.344939708709717, Valid loss 11.394454002380371, Test loss 11.223187446594238\n",
            "Epoch 14599, Train loss 6.344939231872559, Valid loss 11.394454002380371, Test loss 11.223187446594238\n",
            "Epoch 14799, Train loss 6.344939708709717, Valid loss 11.394454002380371, Test loss 11.223187446594238\n",
            "Epoch 14999, Train loss 6.344939708709717, Valid loss 11.394454002380371, Test loss 11.223187446594238\n",
            "Epoch 199, Train loss 14.146740913391113, Valid loss 15.4760103225708, Test loss 15.195836067199707\n",
            "Epoch 399, Train loss 13.358284950256348, Valid loss 15.451284408569336, Test loss 15.076181411743164\n",
            "Epoch 599, Train loss 12.614117622375488, Valid loss 15.169524192810059, Test loss 14.810118675231934\n",
            "Epoch 799, Train loss 11.967608451843262, Valid loss 14.971734046936035, Test loss 14.568336486816406\n",
            "Epoch 999, Train loss 11.25853443145752, Valid loss 14.82039737701416, Test loss 14.348671913146973\n",
            "Epoch 1199, Train loss 10.611328125, Valid loss 14.349535942077637, Test loss 13.915867805480957\n",
            "Epoch 1399, Train loss 9.98209285736084, Valid loss 14.139283180236816, Test loss 13.614941596984863\n",
            "Epoch 1599, Train loss 9.400957107543945, Valid loss 13.8285493850708, Test loss 13.29239559173584\n",
            "Epoch 1799, Train loss 8.881673812866211, Valid loss 13.501742362976074, Test loss 12.964356422424316\n",
            "Epoch 1999, Train loss 8.411890983581543, Valid loss 13.24594497680664, Test loss 12.644082069396973\n",
            "Epoch 2199, Train loss 8.008866310119629, Valid loss 13.064129829406738, Test loss 12.443758964538574\n",
            "Epoch 2399, Train loss 7.669273376464844, Valid loss 12.715362548828125, Test loss 12.122671127319336\n",
            "Epoch 2599, Train loss 7.391024589538574, Valid loss 12.580801963806152, Test loss 11.955992698669434\n",
            "Epoch 2799, Train loss 7.165282726287842, Valid loss 12.515448570251465, Test loss 11.866934776306152\n",
            "Epoch 2999, Train loss 6.975656032562256, Valid loss 12.280835151672363, Test loss 11.662187576293945\n",
            "Epoch 3199, Train loss 6.815890789031982, Valid loss 12.143378257751465, Test loss 11.543746948242188\n",
            "Epoch 3399, Train loss 6.685744285583496, Valid loss 12.131306648254395, Test loss 11.488640785217285\n",
            "Epoch 3599, Train loss 6.578335285186768, Valid loss 12.054685592651367, Test loss 11.432177543640137\n",
            "Epoch 3799, Train loss 6.491610527038574, Valid loss 12.005324363708496, Test loss 11.368260383605957\n",
            "Epoch 3999, Train loss 6.420613765716553, Valid loss 11.92060375213623, Test loss 11.279461860656738\n",
            "Epoch 4199, Train loss 6.361002445220947, Valid loss 11.9096040725708, Test loss 11.28382396697998\n",
            "Epoch 4399, Train loss 6.313421249389648, Valid loss 11.85637378692627, Test loss 11.2379789352417\n",
            "Epoch 4599, Train loss 6.274017333984375, Valid loss 11.831206321716309, Test loss 11.208334922790527\n",
            "Epoch 4799, Train loss 6.242201805114746, Valid loss 11.81626033782959, Test loss 11.188054084777832\n",
            "Epoch 4999, Train loss 6.2165045738220215, Valid loss 11.788378715515137, Test loss 11.163976669311523\n",
            "Epoch 5199, Train loss 6.1946892738342285, Valid loss 11.7793607711792, Test loss 11.151618003845215\n",
            "Epoch 5399, Train loss 6.176713466644287, Valid loss 11.784187316894531, Test loss 11.157271385192871\n",
            "Epoch 5599, Train loss 6.162502288818359, Valid loss 11.762080192565918, Test loss 11.138291358947754\n",
            "Epoch 5799, Train loss 6.150507926940918, Valid loss 11.758695602416992, Test loss 11.130349159240723\n",
            "Epoch 5999, Train loss 6.141048908233643, Valid loss 11.749709129333496, Test loss 11.123467445373535\n",
            "Epoch 6199, Train loss 6.1332550048828125, Valid loss 11.7488374710083, Test loss 11.119269371032715\n",
            "Epoch 6399, Train loss 6.1267313957214355, Valid loss 11.738468170166016, Test loss 11.111067771911621\n",
            "Epoch 6599, Train loss 6.1216206550598145, Valid loss 11.735123634338379, Test loss 11.108107566833496\n",
            "Epoch 6799, Train loss 6.117246150970459, Valid loss 11.736163139343262, Test loss 11.106904029846191\n",
            "Epoch 6999, Train loss 6.113773822784424, Valid loss 11.736958503723145, Test loss 11.107029914855957\n",
            "Epoch 7199, Train loss 6.1108717918396, Valid loss 11.736749649047852, Test loss 11.105549812316895\n",
            "Epoch 7399, Train loss 6.108401298522949, Valid loss 11.727782249450684, Test loss 11.098302841186523\n",
            "Epoch 7599, Train loss 6.106436252593994, Valid loss 11.726187705993652, Test loss 11.096146583557129\n",
            "Epoch 7799, Train loss 6.1048665046691895, Valid loss 11.722163200378418, Test loss 11.092469215393066\n",
            "Epoch 7999, Train loss 6.103578090667725, Valid loss 11.722207069396973, Test loss 11.092740058898926\n",
            "Epoch 8199, Train loss 6.10251522064209, Valid loss 11.721360206604004, Test loss 11.09199047088623\n",
            "Epoch 8399, Train loss 6.10164213180542, Valid loss 11.722579002380371, Test loss 11.092241287231445\n",
            "Epoch 8599, Train loss 6.1009297370910645, Valid loss 11.722164154052734, Test loss 11.091533660888672\n",
            "Epoch 8799, Train loss 6.100330352783203, Valid loss 11.721606254577637, Test loss 11.09134578704834\n",
            "Epoch 8999, Train loss 6.09988260269165, Valid loss 11.72103214263916, Test loss 11.090887069702148\n",
            "Epoch 9199, Train loss 6.099527835845947, Valid loss 11.720773696899414, Test loss 11.090620994567871\n",
            "Epoch 9399, Train loss 6.099236488342285, Valid loss 11.721261024475098, Test loss 11.090938568115234\n",
            "Epoch 9599, Train loss 6.099034309387207, Valid loss 11.720993041992188, Test loss 11.090811729431152\n",
            "Epoch 9799, Train loss 6.0988874435424805, Valid loss 11.721013069152832, Test loss 11.090849876403809\n",
            "Epoch 9999, Train loss 6.098782062530518, Valid loss 11.720742225646973, Test loss 11.090658187866211\n",
            "Epoch 10199, Train loss 6.098710536956787, Valid loss 11.720754623413086, Test loss 11.090657234191895\n",
            "Epoch 10399, Train loss 6.098664283752441, Valid loss 11.720765113830566, Test loss 11.090648651123047\n",
            "Epoch 10599, Train loss 6.0986328125, Valid loss 11.720784187316895, Test loss 11.090641975402832\n",
            "Epoch 10799, Train loss 6.098611831665039, Valid loss 11.72078800201416, Test loss 11.090632438659668\n",
            "Epoch 10999, Train loss 6.098598003387451, Valid loss 11.720746994018555, Test loss 11.090607643127441\n",
            "Epoch 11199, Train loss 6.098588466644287, Valid loss 11.720717430114746, Test loss 11.090579986572266\n",
            "Epoch 11399, Train loss 6.0985822677612305, Valid loss 11.720725059509277, Test loss 11.090584754943848\n",
            "Epoch 11599, Train loss 6.098577976226807, Valid loss 11.720714569091797, Test loss 11.090576171875\n",
            "Epoch 11799, Train loss 6.098575115203857, Valid loss 11.72071647644043, Test loss 11.090575218200684\n",
            "Epoch 11999, Train loss 6.098573684692383, Valid loss 11.72071361541748, Test loss 11.090571403503418\n",
            "Epoch 12199, Train loss 6.09857177734375, Valid loss 11.720708847045898, Test loss 11.090567588806152\n",
            "Epoch 12399, Train loss 6.098571300506592, Valid loss 11.720707893371582, Test loss 11.090567588806152\n",
            "Epoch 12599, Train loss 6.098570823669434, Valid loss 11.720706939697266, Test loss 11.090566635131836\n",
            "Epoch 12799, Train loss 6.098570823669434, Valid loss 11.72070598602295, Test loss 11.090566635131836\n",
            "Epoch 12999, Train loss 6.098570346832275, Valid loss 11.720706939697266, Test loss 11.09056568145752\n",
            "Epoch 13199, Train loss 6.098570346832275, Valid loss 11.720707893371582, Test loss 11.090566635131836\n",
            "Epoch 13399, Train loss 6.098569393157959, Valid loss 11.720707893371582, Test loss 11.090566635131836\n",
            "Epoch 13599, Train loss 6.098569869995117, Valid loss 11.720707893371582, Test loss 11.090566635131836\n",
            "Epoch 13799, Train loss 6.098569393157959, Valid loss 11.720706939697266, Test loss 11.090566635131836\n",
            "Epoch 13999, Train loss 6.098569869995117, Valid loss 11.720707893371582, Test loss 11.090566635131836\n",
            "Epoch 14199, Train loss 6.098569393157959, Valid loss 11.720707893371582, Test loss 11.090566635131836\n",
            "Epoch 14399, Train loss 6.098569393157959, Valid loss 11.720706939697266, Test loss 11.090566635131836\n",
            "Epoch 14599, Train loss 6.098569393157959, Valid loss 11.72070598602295, Test loss 11.09056568145752\n",
            "Epoch 14799, Train loss 6.098569393157959, Valid loss 11.720706939697266, Test loss 11.09056568145752\n",
            "Epoch 14999, Train loss 6.098569393157959, Valid loss 11.72070598602295, Test loss 11.09056568145752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_system(x, u):\n",
        "  ensemble_mean = np.zeros((1,14))\n",
        "  #ensemble_mean = torch.from_numpy(ensemble_mean)\n",
        "  #ensemble_mean = ensemble_mean.cuda()\n",
        "  ensemble_var = np.zeros((1,14))\n",
        "  #ensemble_var = torch.from_numpy(ensemble_var)\n",
        "  #ensemble_var = ensemble_var.cuda()\n",
        "\n",
        "  x = x.T\n",
        "  u = u.T\n",
        "  x = np.hstack((x,u))\n",
        "  x = x.astype(np.float32)\n",
        "  #x[7:14]=x[7:14]/8\n",
        "  #x = torch.from_numpy(x)\n",
        "  x=x.reshape(1, -1)\n",
        "  #scaler = StandardScaler().fit(x)\n",
        "  x[:,7:14] = scalerX.transform(x[:,7:14])\n",
        "  for model in ensemble:\n",
        "    mean, var = model.forward(x, \"nparray\")\n",
        "    mean = mean.cpu()\n",
        "    var = var.cpu()\n",
        "    mean = mean.detach().numpy()\n",
        "    mean[:,-7:] = scalery.inverse_transform(mean[:,-7:])\n",
        "    var = var.detach().numpy()\n",
        "    ensemble_mean += mean\n",
        "    ensemble_var += var + mean ** 2\n",
        "    \n",
        "  ensemble_mean = ensemble_mean / ensemble_size\n",
        "  ensemble_var = ensemble_var / ensemble_size\n",
        "  ensemble_var = ensemble_var - ensemble_mean**2\n",
        "  #ensemble_mean = ensemble_mean.cpu()\n",
        "  #ensemble_var = ensemble_var.cpu()\n",
        "  #ensemble_mean = ensemble_mean.detach().numpy()\n",
        "  #ensemble_mean[0,-7:]=ensemble_mean[0,-7:]*10\n",
        "  #print(ensemble_mean)\n",
        "\n",
        "  return ensemble_mean,ensemble_var"
      ],
      "metadata": {
        "id": "6D88i6xdWY-E"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_system(x, u):\n",
        "  ensemble_mean = np.zeros((1,14))\n",
        "  #ensemble_mean = torch.from_numpy(ensemble_mean)\n",
        "  #ensemble_mean = ensemble_mean.cuda()\n",
        "  ensemble_var = np.zeros((1,14))\n",
        "  #ensemble_var = torch.from_numpy(ensemble_var)\n",
        "  #ensemble_var = ensemble_var.cuda()\n",
        "\n",
        "  x = x.T\n",
        "  u = u.T\n",
        "  x = np.hstack((x,u))\n",
        "  x = x.astype(np.float32)\n",
        "  #x[7:14]=x[7:14]/8\n",
        "  #x = torch.from_numpy(x)\n",
        "  #x=x.reshape(1, -1)\n",
        "  #scaler = StandardScaler().fit(x)\n",
        "  #x = scalerX.transform(x)\n",
        "  for model in ensemble:\n",
        "    mean, var = model.forward(x, \"nparray\")\n",
        "    mean = mean.cpu()\n",
        "    var = var.cpu()\n",
        "    mean = mean.detach().numpy()\n",
        "    #mean = scalery.inverse_transform(mean)\n",
        "    var = var.detach().numpy()\n",
        "    ensemble_mean += mean\n",
        "    ensemble_var += var + mean ** 2\n",
        "    \n",
        "  ensemble_mean = ensemble_mean / ensemble_size\n",
        "  ensemble_var = ensemble_var / ensemble_size\n",
        "  ensemble_var = ensemble_var - ensemble_mean**2\n",
        "  #ensemble_mean = ensemble_mean.cpu()\n",
        "  #ensemble_var = ensemble_var.cpu()\n",
        "  #ensemble_mean = ensemble_mean.detach().numpy()\n",
        "  #ensemble_mean[0,-7:]=ensemble_mean[0,-7:]*10\n",
        "  #print(ensemble_mean)\n",
        "\n",
        "  return ensemble_mean,ensemble_var"
      ],
      "metadata": {
        "id": "A6cdvzk1UZLk"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num = random.sample(range(1, 1000), 100)\n",
        "output = 0\n",
        "for i in num:\n",
        "  ZZ = np.array(test_data[i,:14])\n",
        "  UU = np.array(test_data[i,14:21])\n",
        "  mean, var = simulate_system(ZZ, UU)\n",
        "  mseloss = nn.MSELoss()\n",
        "  target = torch.from_numpy(test_data[i,21:])\n",
        "  target = target.view(-1,14)\n",
        "  target = target.cuda()\n",
        "  output += mseloss(torch.from_numpy(mean).cuda(), target)\n",
        "output = output / 100\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6ZMzi7LUUuR",
        "outputId": "018ad856-ec13-4385-d347-9c2d7f9bc039"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.7021, device='cuda:0', dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num = random.sample(range(1, 1000), 100)\n",
        "output = 0\n",
        "for i in num:\n",
        "  ZZ = np.array(testt_data[i,:14])\n",
        "  UU = np.array(testt_data[i,14:21])\n",
        "  mean, var = simulate_system(ZZ, UU)\n",
        "  mseloss = nn.MSELoss()\n",
        "  target = torch.from_numpy(testt_data[i,21:])\n",
        "  target = target.view(-1,14)\n",
        "  target = target.cuda()\n",
        "  output += mseloss(torch.from_numpy(mean).cuda(), target)\n",
        "output = output / 100\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-9A5RSkbUco",
        "outputId": "d2eb64d6-2b59-496a-e694-fb69e20762ff"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(155.1498, device='cuda:0', dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ZZ = np.array(test_data[10,:14])\n",
        "UU = np.array(test_data[10,14:21])\n",
        "mean, var = simulate_system(ZZ, UU)\n",
        "mseloss = nn.MSELoss()\n",
        "target = torch.from_numpy(test_data[10,21:])\n",
        "target = target.view(-1,14)\n",
        "target = target.cuda()\n",
        "output = mseloss(torch.from_numpy(mean).cuda(), target)\n",
        "print(mean)\n",
        "#mean[0,-7:]=mean[0,-7:]*8\n",
        "#print(mean)\n",
        "print(test_data[10,21:])\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hfi0DAlXqza",
        "outputId": "6490112c-716b-4280-f656-feb34246ea51"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.41994518  0.32729205  0.18088663 -0.27017845  0.29406992  0.1758893\n",
            "   0.19786687  0.29198781 -0.10809198  0.16674505  0.51833213  0.52414673\n",
            "   0.1959481   0.3832396 ]]\n",
            "[-0.12915713  1.6808405   1.0073076  -1.5112218   1.9002388   0.6560098\n",
            "  0.20049791  0.46418228 -1.56467176 -0.49174439  0.75359988  1.14326798\n",
            "  0.52984028  1.20908749]\n",
            "tensor(0.7691, device='cuda:0', dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "   \n",
        "def get_linearization(current_state, control_signal):\n",
        "    eps = 1e-5\n",
        "    A = np.zeros([len(current_state), len(current_state)])\n",
        "    for ii in range(len(current_state)):\n",
        "        x = current_state.copy()\n",
        "        x[ii] += eps\n",
        "        x_inc,varr = simulate_system(x, control_signal)  \n",
        "        x = current_state.copy()\n",
        "        x[ii] -= eps\n",
        "        x_dec,varr = simulate_system(x, control_signal)\n",
        "        A[:,ii] = (x_inc - x_dec) / (2 * eps)\n",
        " \n",
        "    B = np.zeros([len(current_state), len(control_signal)])\n",
        "    for ii in range(len(control_signal)):\n",
        "        u = control_signal.copy()\n",
        "        u[ii] += eps\n",
        "        x_inc,varr = simulate_system(current_state, u)\n",
        "        u = control_signal.copy()\n",
        "        u[ii] -= eps\n",
        "        x_dec,varr = simulate_system(current_state, u)\n",
        "        B[:,ii] = (x_inc - x_dec) / (2 * eps)\n",
        "    return A, B\n",
        "\n",
        "def solve_ricatti_equations(Z,U,Q,R,N,q,r):\n",
        "    \n",
        "    K_gains = []\n",
        "    k_feedforward = []\n",
        "    S = Q[-1]\n",
        "    s = q[-1]\n",
        "       \n",
        "    for i in reversed(range(horizon_length)):\n",
        "        A, B = get_linearization(Z[:,i], U[:,i])\n",
        "        W = np.linalg.inv(Sigma*var)-sigma*C.T.dot(S).dot(C)\n",
        "        iW = np.linalg.inv(W)\n",
        "        term_covar = C.dot(iW).dot(C.T)\n",
        "        H = R + B.T.dot(S).dot(B) + sigma * B.T.dot(S.T).dot(term_covar).dot(S).dot(B)\n",
        "        g = r[i] + B.T.dot(s) + sigma * B.T.dot(S.T).dot(term_covar).dot(s)\n",
        "        G = B.T.dot(S).dot(A) + sigma * B.T.dot(S.T).dot(term_covar.T).dot(S).dot(A)\n",
        "        k = -np.linalg.solve(H,g)\n",
        "        K = -np.linalg.solve(H,G)\n",
        "        s = q[i] + A.T.dot(s) + G.T.dot(k) + K.T.dot(g) + K.T.dot(H).dot(k) + sigma * A.T.dot(S.T).dot(term_covar).dot(s)\n",
        "        S = Q[i] + A.T.dot(S).dot(A) + K.T.dot(H).dot(K) + G.T.dot(K) + K.T.dot(G) + sigma * A.T.dot(S.T).dot(term_covar).dot(S).dot(A)\n",
        "      \n",
        "        K_gains.append(K)\n",
        "        k_feedforward.append(k)\n",
        "        \n",
        "        \n",
        "       \n",
        "    K_gains = K_gains[::-1]\n",
        "    k_feedforward = k_feedforward[::-1]\n",
        "    return K_gains, k_feedforward\n",
        "\n",
        "def controller(Z,U,q,r,horizon_length,Q,R,alpha):\n",
        "    state2=np.empty([14, horizon_length+1])\n",
        "    z0 = np.zeros([14,])\n",
        "    state2[:,0] = z0\n",
        "    u2 = np.zeros([7, horizon_length])\n",
        "    K,k = solve_ricatti_equations(Z,U,Q,R,horizon_length,q,r)\n",
        "    for i in range(horizon_length):\n",
        "        u2[:,i] = U[:,i] + K[i] @ (state2[:,i] - Z[:,i])+ alpha*k[i]\n",
        "        state2[:,i+1],var = simulate_system(state2[:,i], u2[:,i])\n",
        "    state=state2.copy()\n",
        "    u=u2.copy()\n",
        "    return state,u,k\n",
        "\n",
        "def compute_cost(Z,U,z_bar,u_bar, horizon_length):\n",
        "    J=((Z[:,horizon_length]-z_bar[:,horizon_length]).T)@ Q[horizon_length] @(Z[:,horizon_length]-z_bar[:,horizon_length])\n",
        "    for i in range(horizon_length):\n",
        "        J=J+((Z[:,i]-z_bar[:,i]).T)@ Q[i] @(Z[:,i]-z_bar[:,i])+((U[:,i]-u_bar[:,i]).T @ R @ (U[:,i]-u_bar[:,i]))\n",
        "    \n",
        "    return J\n",
        "\n",
        "def get_quadratic_approximation_cost(Z,U, horizon_length,z_bar,u_bar):\n",
        "    q=[]\n",
        "    r=[]\n",
        "    J=compute_cost(Z,U,z_bar,u_bar, horizon_length)\n",
        "    #J=J+0.5*((z[:,horizon_length]-Z[:,horizon_length]).T)@ Q[horizon_length] @(z[:,horizon_length]-Z[:,horizon_length])+2*(z[:,horizon_length].T-z_bar[:,horizon_length].T)@Q[horizon_length]@(z[:,horizon_length]-Z[:,horizon_length])\n",
        "    for i in range(horizon_length+1):\n",
        "        #J=J+0.5*((u[:,i]-U[:,i]).T)@ R @(u[:,i]-U[:,i])+0.5*((z[:,i]-Z[:,i]).T)@ Q[i] @(z[:,i]-Z[:,i])+2*((z[:,i]-z_bar[:,i]).T)@Q[i]@(z[:,i]-Z[:,i])+2*((u[:,i]-u_bar[:,i]).T)@R@(u[:,i]-U[:,i])    \n",
        "        q1=2*(Z[:,i]-z_bar[:,i]).T@Q[i]\n",
        "        q.append(q1)\n",
        "    for i in range(horizon_length):\n",
        "        r1=2*(U[:,i]-u_bar[:,i]).T@R\n",
        "        r.append(r1)\n",
        "    \n",
        "    return q,r"
      ],
      "metadata": {
        "id": "8tWLfPyKqljM"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=1\n",
        "horizon_length = 40\n",
        "Q2=0.00001*np.eye(14)\n",
        "Q1=30*np.eye(14)\n",
        "for i in range(7,14):\n",
        "    Q1[i,i]=0.00001\n",
        "Q=[]\n",
        "for i in range(horizon_length+1):\n",
        "    if (i==horizon_length):\n",
        "        Q.append(Q1)\n",
        "    else:\n",
        "        Q.append(Q2)\n",
        "\n",
        "R=0.00001*np.eye(7)\n",
        "sigma=0\n",
        "C=np.eye(14)\n",
        "Sigma=1*np.eye(14)\n",
        "var = np.ones((14,1))\n",
        "\n",
        "\n",
        "u_bar = np.zeros([7, horizon_length])\n",
        "U = np.zeros([7, horizon_length])\n",
        "Z = np.zeros([14,horizon_length+1])\n",
        "u= np.zeros([7, horizon_length])\n",
        "    \n",
        "      \n",
        "z_bar=np.zeros([14,horizon_length+1])\n",
        "z_bar[0:7,horizon_length]=np.array([-1.8336430453226058, -1.4900804580013904, -0.33423961967431476, -1.108836361791467, 0.7815563306180698, 0.5042580934421823, 1.1407696303315698])"
      ],
      "metadata": {
        "id": "qWjdVEBlqRLk"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q,r=get_quadratic_approximation_cost(Z,U, horizon_length,z_bar,u_bar)\n",
        "state,u,k = controller(Z,U,q,r,horizon_length,Q,R,a)\n",
        "U = u\n",
        "Z = np.zeros([14, horizon_length+1])\n",
        "for i in range(horizon_length):\n",
        "    Z[:,i+1],var = simulate_system(Z[:,i], U[:,i])\n",
        "J=compute_cost(Z,U,z_bar,u_bar, horizon_length)\n",
        "J1=J\n",
        "print(J)\n",
        "\n",
        "q,r=get_quadratic_approximation_cost(Z,U, horizon_length,z_bar,u_bar)\n",
        "state,u,k = controller(Z,U,q,r,horizon_length,Q,R,a)\n",
        "U = u\n",
        "Z = np.zeros([14, horizon_length+1])\n",
        "for i in range(horizon_length):\n",
        "    Z[:,i+1],var = simulate_system(Z[:,i], U[:,i])\n",
        "J=compute_cost(Z,U,z_bar,u_bar, horizon_length)\n",
        "J1=J\n",
        "print(J)\n",
        "\n",
        "for j in range(300):\n",
        "    q,r=get_quadratic_approximation_cost(Z,U, horizon_length,z_bar,u_bar)\n",
        "    state,u,k = controller(Z,U,q,r,horizon_length,Q,R,a)\n",
        "    U = u\n",
        "    Z = np.zeros([14, horizon_length+1])\n",
        "    for i in range(horizon_length):\n",
        "        Z[:,i+1],var = simulate_system(Z[:,i], U[:,i])\n",
        "    J=compute_cost(Z,U,z_bar,u_bar, horizon_length)\n",
        "    print(J)\n",
        "    if (J<=J1):\n",
        "        if (abs(J1-J)<1e-3):\n",
        "            print(\"iteration converged\")\n",
        "            break\n",
        "        J1=J\n",
        "    else:\n",
        "        a=a/2\n",
        "        J1=J\n",
        "        print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFEbS5Ptp1oX",
        "outputId": "749a4a21-a5b4-48f9-ba26-2808ca734442"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1764.0940700551573\n",
            "4368.444221190466\n",
            "3016.3487242705487\n",
            "10941.381894470414\n",
            "0.5\n",
            "568.9304186291665\n",
            "131.0712586501402\n",
            "220.08256641730205\n",
            "0.25\n",
            "244.69857251536789\n",
            "0.125\n",
            "220.22380135918695\n",
            "266.8308866436493\n",
            "0.0625\n",
            "284.0212085196207\n",
            "0.03125\n",
            "267.773492788985\n",
            "278.60551719244836\n",
            "0.015625\n",
            "280.57219031950063\n",
            "0.0078125\n",
            "270.4187906811664\n",
            "269.1470017977944\n",
            "267.8582210214736\n",
            "256.17405706145536\n",
            "255.14145076485676\n",
            "260.583483116799\n",
            "0.00390625\n",
            "261.3624641871963\n",
            "0.001953125\n",
            "259.72128076337094\n",
            "259.85643485569904\n",
            "0.0009765625\n",
            "259.9263921952759\n",
            "0.00048828125\n",
            "259.87635809439837\n",
            "259.8696809061955\n",
            "259.66754197437916\n",
            "259.5599291275139\n",
            "259.3371595081322\n",
            "259.1546820786226\n",
            "260.98470707253847\n",
            "0.000244140625\n",
            "260.98283231958544\n",
            "261.0395155431853\n",
            "0.0001220703125\n",
            "261.050684228559\n",
            "6.103515625e-05\n",
            "261.05371294193156\n",
            "3.0517578125e-05\n",
            "261.04843031828074\n",
            "261.05280145709946\n",
            "1.52587890625e-05\n",
            "261.0517195434448\n",
            "261.0473686459979\n",
            "261.0445452199423\n",
            "261.04289854729853\n",
            "261.058823144422\n",
            "7.62939453125e-06\n",
            "261.0584115994157\n",
            "iteration converged\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Z[:,-1])\n",
        "print(z_bar[:,-1])\n",
        "target = torch.from_numpy(z_bar[:7,-1])\n",
        "target = target.view(-1,7)\n",
        "target = target.cuda()\n",
        "mean = torch.from_numpy(Z[:7,-1])\n",
        "mean = mean.view(-1,7)\n",
        "mean = mean.cuda()\n",
        "mseloss = nn.MSELoss()\n",
        "output = mseloss(mean, target)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwQwX0iMt99D",
        "outputId": "16774a26-e64a-4872-d955-9f21d20c130b"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  -1.51067228   -0.95290516   -0.98236871   -0.92318048    0.8210994\n",
            "    0.29714266   -1.63592916   23.9194169   -15.69344282   -8.43100029\n",
            "  -22.33611989  -30.81946517  -23.94942093 -291.59596252]\n",
            "[-1.83364305 -1.49008046 -0.33423962 -1.10883636  0.78155633  0.50425809\n",
            "  1.14076963  0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "tensor(1.2288, device='cuda:0', dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(u)\n",
        "dataframe = pd.DataFrame(u)\n",
        "dataframe.to_csv(\"u.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84l1SNWY5Zmi",
        "outputId": "bcbc3e1a-b438-4426-8ff3-890664386420"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-2.17296983e-01  4.02726727e-02 -7.08119037e-02 -7.19748040e-02\n",
            "  -5.77669493e-02 -3.33884855e-02 -8.09337247e-02 -8.60744397e-02\n",
            "  -1.66931416e-02 -3.27173970e-02 -1.04147333e-01  9.81321878e-03\n",
            "  -7.67249304e-02 -2.70699550e-01 -9.92097876e-02 -1.01524569e-01\n",
            "   3.41869537e-02  1.29421614e-01 -2.17952808e-01 -7.03779222e-02\n",
            "  -7.61003955e-02  8.85561968e-02 -2.05930568e-01 -1.06543660e-01\n",
            "  -8.61523103e-02 -9.46866606e-02  4.02934929e-02 -9.34032222e-02\n",
            "   2.36215156e-01  6.84697137e-02  3.63036110e-02 -6.46248846e-02\n",
            "   9.24341183e-02 -8.23033204e-01 -2.30154720e+00  5.68644437e+00\n",
            "   1.58934743e+00  3.06261865e+01  1.63977679e+01  7.06668199e+01]\n",
            " [-2.93129200e-01 -2.92823410e-01 -2.16629683e-01 -2.25572502e-01\n",
            "  -2.22326329e-01 -4.56418088e-01 -2.33908710e-01 -2.20708252e-01\n",
            "  -2.98240580e-01 -2.55848540e-01 -2.44725268e-01 -2.67551721e-01\n",
            "  -2.27170786e-01 -2.40707666e-01 -2.35030796e-01 -2.40236345e-01\n",
            "  -1.66642738e-01 -4.48256466e-01 -1.96291302e-01 -2.51326100e-01\n",
            "  -2.36494960e-01 -2.69745218e-01 -2.04675380e-01 -2.40031097e-01\n",
            "  -2.35756360e-01 -2.24075345e-01 -4.47262152e-01 -1.83340949e-01\n",
            "  -3.35589187e-01 -3.02914122e-01 -4.61369575e-01 -2.07677359e-01\n",
            "  -6.02066484e-01 -8.67085579e-01 -8.88170765e-01  5.66708924e+00\n",
            "  -1.19487391e+00  2.34790782e+01  3.17549319e+01 -4.01272257e+01]\n",
            " [ 5.19150959e-02  1.56717334e-01  6.34973690e-02  6.60207672e-02\n",
            "   8.85926330e-02  5.39814353e-02  6.24384560e-02  7.74426235e-02\n",
            "   9.28418758e-02  7.52676640e-02  6.97954988e-02  1.17417306e-01\n",
            "   9.53310762e-02  6.24664580e-02  9.63075652e-02  9.14937032e-02\n",
            "   1.39812181e-01  8.22447822e-02  5.12476883e-02  8.17492704e-02\n",
            "   5.44949547e-02  2.06225048e-01  5.40724991e-02  6.16892926e-02\n",
            "   5.44247856e-02  8.48277337e-02  8.01469470e-02  1.02212651e-01\n",
            "   1.85096470e-01  1.46770393e-01  5.65811444e-02  9.64396226e-02\n",
            "  -1.08733222e-01  2.53169694e-01  7.17988761e-01  2.94536124e+00\n",
            "  -3.20164388e+01  4.70915931e+01 -1.64021291e+00 -1.52387681e+02]\n",
            " [-1.18171831e-01  1.57973685e-02 -1.86816263e-01 -1.88766002e-01\n",
            "  -1.74481260e-01  7.30693014e-02 -1.91591295e-01 -1.95412434e-01\n",
            "  -1.57121487e-01 -1.92625074e-01 -2.07159159e-01 -2.07319556e-01\n",
            "  -1.81625613e-01 -1.50402173e-01 -1.90701681e-01 -1.90590389e-01\n",
            "  -1.86987836e-01  3.29032408e-02 -1.66580281e-01 -1.82234354e-01\n",
            "  -1.94815445e-01 -4.13930091e-01 -1.69986236e-01 -2.07687916e-01\n",
            "  -1.92606003e-01 -1.97719632e-01  2.41967682e-01 -1.72490726e-01\n",
            "  -1.05958107e-01 -2.66626725e-01  8.74629432e-02 -2.21445555e-01\n",
            "   1.80604551e-01 -6.33755489e-02  1.51753038e-01 -1.57032444e+00\n",
            "   2.79811302e+00 -7.34423222e+01  5.85287416e+01 -9.55730908e+01]\n",
            " [-5.40314456e-01 -1.97008745e-01 -4.21506346e-01 -4.20744582e-01\n",
            "  -3.98340433e-01 -1.74690785e-01 -4.25987932e-01 -4.37008223e-01\n",
            "  -3.92765260e-01 -4.02311850e-01 -4.58010293e-01 -3.69724630e-01\n",
            "  -4.21447668e-01 -5.10563130e-01 -4.49300315e-01 -4.47801237e-01\n",
            "  -2.85232264e-01 -7.82782037e-02 -4.96008380e-01 -4.22859004e-01\n",
            "  -4.28826381e-01 -3.53496863e-01 -4.84440649e-01 -4.62714607e-01\n",
            "  -4.33981106e-01 -4.51114072e-01  2.04104546e-01 -4.02851793e-01\n",
            "  -1.64559480e-01 -3.29398707e-01 -9.56694662e-02 -4.32034917e-01\n",
            "  -1.06263343e-01 -7.81881567e-01 -1.06672409e+00  2.02933040e+00\n",
            "   1.93205527e+01 -5.89246889e+00  2.34164683e+01 -1.49364615e+02]\n",
            " [ 1.20324124e+00  9.49930653e-01  9.62668659e-01  9.29384303e-01\n",
            "   9.34754447e-01  7.11165913e-01  9.40511424e-01  9.11118387e-01\n",
            "   9.28578007e-01  9.37669313e-01  8.89142042e-01  9.28872883e-01\n",
            "   8.87092978e-01  8.95752373e-01  8.68923651e-01  8.63941301e-01\n",
            "   9.43896151e-01  7.32748236e-01  9.31658855e-01  9.08709237e-01\n",
            "   9.34727128e-01  8.99116871e-01  9.07790867e-01  9.03886701e-01\n",
            "   9.28332735e-01  8.87461560e-01  5.61230258e-01  9.45965411e-01\n",
            "   8.16273747e-01  9.15504130e-01  6.45587248e-01  9.16957835e-01\n",
            "   5.87907024e-01  8.59935107e-01  1.91555640e-01 -4.90015425e+00\n",
            "  -1.00208041e+01 -8.02806907e+01  5.36469673e+00 -2.53055859e+02]\n",
            " [ 1.96210572e+00  2.11633425e+00  2.09292728e+00  2.09087046e+00\n",
            "   2.11402313e+00  2.17277178e+00  2.09928216e+00  2.12702415e+00\n",
            "   2.12743495e+00  2.12290321e+00  2.10637496e+00  2.16102190e+00\n",
            "   2.08845010e+00  2.07156302e+00  2.11818633e+00  2.12829110e+00\n",
            "   2.15970094e+00  2.24200130e+00  2.12120606e+00  2.11900640e+00\n",
            "   2.10731794e+00  2.16584932e+00  2.11949652e+00  2.11797107e+00\n",
            "   2.11974392e+00  2.12213283e+00  2.23080396e+00  2.11314855e+00\n",
            "   2.19550142e+00  2.15302919e+00  2.22917795e+00  2.11404961e+00\n",
            "   2.21065594e+00  2.00730435e+00  2.06143064e+00 -6.27245679e+00\n",
            "  -3.31886728e+01 -3.53207500e+01  6.98923184e+01 -1.91193823e+02]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------"
      ],
      "metadata": {
        "id": "V91izoO-tdvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q,r=get_quadratic_approximation_cost(Z,U, horizon_length,z_bar,u_bar)\n",
        "state,u,k = controller(Z,U,q,r,horizon_length,Q,R,a)\n",
        "U = u\n",
        "Z = np.zeros([14, horizon_length+1])\n",
        "for i in range(horizon_length):\n",
        "    Z[:,i+1],var = simulate_system(Z[:,i], U[:,i])\n",
        "J=compute_cost(Z,U,z_bar,u_bar, horizon_length)\n",
        "J1=J\n",
        "print(J)\n",
        "\n",
        "for j in range(100):\n",
        "    q,r=get_quadratic_approximation_cost(Z,U, horizon_length,z_bar,u_bar)\n",
        "    state,u,k = controller(Z,U,q,r,horizon_length,Q,R,a)\n",
        "    U = u\n",
        "    Z = np.zeros([14, horizon_length+1])\n",
        "    \n",
        "    for i in range(horizon_length):\n",
        "        Z[:,i+1],var = simulate_system(Z[:,i], U[:,i])\n",
        "    J=compute_cost(Z,U,z_bar,u_bar, horizon_length)\n",
        "    \n",
        "    print(J)\n",
        "    if (J<=J1):\n",
        "        if (abs(J1-J)<1e-2):\n",
        "            print(\"iteration converged\")\n",
        "            break\n",
        "        J1=J\n",
        "    else:\n",
        "        a=a/2\n",
        "        J1=J\n",
        "        print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "HvI4fnCuYFct",
        "outputId": "227a10c3-160a-4d55-cd11-6b34f61685cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nan\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-aba7dd064c6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_quadratic_approximation_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhorizon_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontroller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhorizon_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhorizon_length\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-7915a881d820>\u001b[0m in \u001b[0;36mcontroller\u001b[0;34m(Z, U, q, r, horizon_length, Q, R, alpha)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mstate2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mu2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhorizon_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolve_ricatti_equations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhorizon_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhorizon_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mu2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstate2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-7915a881d820>\u001b[0m in \u001b[0;36msolve_ricatti_equations\u001b[0;34m(Z, U, Q, R, N, q, r)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhorizon_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linearization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;31m#print(ensemble_var)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-7915a881d820>\u001b[0m in \u001b[0;36mget_linearization\u001b[0;34m(current_state, control_signal)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mx_dec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mensemble_varr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimulate_system\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol_signal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_inc\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx_dec\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-4a29a0d4e4fd>\u001b[0m in \u001b[0;36msimulate_system\u001b[0;34m(x, u)\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;31m#x = torch.from_numpy(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nparray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mensemble_mean\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mensemble_var\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-f3c6471a548c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, input_type)\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m       \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0;31m#var = self.softplus(var)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_linearization(current_state, control_signal):\n",
        "    eps = 1e-5\n",
        "    A = np.zeros([len(current_state), len(current_state)])\n",
        "    for ii in range(len(current_state)):\n",
        "        x = current_state.copy()\n",
        "        x[ii] += eps\n",
        "        x_inc,ensemble_varr = simulate_system(x, control_signal) \n",
        "        x = current_state.copy()\n",
        "        x[ii] -= eps\n",
        "        x_dec,ensemble_varr = simulate_system(x, control_signal)\n",
        "        A[:,ii] = (x_inc - x_dec) / (2 * eps)\n",
        " \n",
        "    B = np.zeros([len(current_state), len(control_signal)])\n",
        "    for ii in range(len(control_signal)):\n",
        "        u = control_signal.copy()\n",
        "        u[ii] += eps\n",
        "        x_inc,ensemble_varr = simulate_system(current_state, u)\n",
        "        u = control_signal.copy()\n",
        "        u[ii] -= eps\n",
        "        x_dec,ensemble_varr = simulate_system(current_state, u)\n",
        "        B[:,ii] = (x_inc - x_dec) / (2 * eps)\n",
        "    return A, B\n",
        "\n",
        "def solve_ricatti_equations(Z,U,Q,R,N,q,r):\n",
        "    \n",
        "    K_gains = []\n",
        "    k_feedforward = []\n",
        "    S = Q[-1].copy()\n",
        "    s = q[-1].copy()\n",
        "       \n",
        "    for i in reversed(range(horizon_length)):\n",
        "        A, B = get_linearization(Z[:,i], U[:,i])\n",
        "        #print(ensemble_var)\n",
        "        W = np.linalg.inv(Sigma)-sigma*C.T.dot(S).dot(C)\n",
        "        #print(\"W\",W)\n",
        "        iW = np.linalg.inv(W)\n",
        "        #print(\"iW\",iW)\n",
        "        term_covar = C.dot(iW).dot(C.T)\n",
        "        H = R + B.T.dot(S).dot(B) + sigma * B.T.dot(S.T).dot(term_covar).dot(S).dot(B)\n",
        "        g = r[i] + B.T.dot(s) + sigma * B.T.dot(S.T).dot(term_covar).dot(s)\n",
        "        G = B.T.dot(S).dot(A) + sigma * B.T.dot(S.T).dot(term_covar.T).dot(S).dot(A)\n",
        "        k = -np.linalg.solve(H,g)\n",
        "        K = -np.linalg.solve(H,G)\n",
        "        s = q[i] + A.T.dot(s) + G.T.dot(k) + K.T.dot(g) + K.T.dot(H).dot(k) + sigma * A.T.dot(S.T).dot(term_covar).dot(s)\n",
        "        #print(\"term_covar\",term_covar)\n",
        "        #print(\"A.T.dot(S).dot(A)\",A.T.dot(S).dot(A))\n",
        "        #print(\"K.T.dot(H).dot(K)\",K.T.dot(H).dot(K))\n",
        "        #print(\"G.T.dot(K)\",G.T.dot(K))\n",
        "        #print(\"K.T.dot(G)\",K.T.dot(G))\n",
        "        #print(\"A\",A)\n",
        "        #print(\"S\",S)\n",
        "        #print(\"K\",K)\n",
        "        #print(\"G\",G)\n",
        "        #print(\"Q[i]\",Q[i])\n",
        "        #print(\"term_covar\",term_covar)\n",
        "        #print(\"sigma * A.T.dot(S.T).dot(term_covar).dot(S).dot(A)\", A.T.dot(S.T).dot(term_covar).dot(S).dot(A))\n",
        "        S = Q[i] + A.T.dot(S).dot(A) + K.T.dot(H).dot(K) + G.T.dot(K) + K.T.dot(G) + sigma * A.T.dot(S.T).dot(term_covar).dot(S).dot(A)\n",
        "        #print(\"\",S)\n",
        "        K_gains.append(K)\n",
        "        k_feedforward.append(k)\n",
        "        \n",
        "       \n",
        "    K_gains = K_gains[::-1]\n",
        "    k_feedforward = k_feedforward[::-1]\n",
        "    return K_gains, k_feedforward\n",
        "\n",
        "def controller(Z,U,q,r,horizon_length,Q,R,alpha):\n",
        "    state2=np.empty([14, horizon_length+1])\n",
        "    z0 = np.zeros([14,])\n",
        "    state2[:,0] = z0\n",
        "    u2 = np.zeros([7, horizon_length])\n",
        "    K,k = solve_ricatti_equations(Z,U,Q,R,horizon_length,q,r)\n",
        "    for i in range(horizon_length):\n",
        "        u2[:,i] = U[:,i] + K[i] @ (state2[:,i] - Z[:,i])+ alpha*k[i]\n",
        "        state2[:,i+1],ensemble_varr = simulate_system(state2[:,i], u2[:,i])\n",
        "   \n",
        "    state=state2.copy()\n",
        "    u=u2.copy()\n",
        "    return state,u,k\n",
        "\n",
        "def compute_cost(Z,U,z_bar,u_bar, horizon_length):\n",
        "    J=((Z[:,horizon_length]-z_bar[:,horizon_length]).T)@ Q[horizon_length] @(Z[:,horizon_length]-z_bar[:,horizon_length])\n",
        "    for i in range(horizon_length):\n",
        "        J=J+((Z[:,i]-z_bar[:,i]).T)@ Q[i] @(Z[:,i]-z_bar[:,i])+((U[:,i]-u_bar[:,i]).T @ R @ (U[:,i]-u_bar[:,i]))\n",
        "    \n",
        "    return J\n",
        "\n",
        "def get_quadratic_approximation_cost(Z,U, horizon_length,z_bar,u_bar):\n",
        "    q=[]\n",
        "    r=[]\n",
        "    J=compute_cost(Z,U,z_bar,u_bar, horizon_length)\n",
        "    #J=J+0.5*((z[:,horizon_length]-Z[:,horizon_length]).T)@ Q[horizon_length] @(z[:,horizon_length]-Z[:,horizon_length])+2*(z[:,horizon_length].T-z_bar[:,horizon_length].T)@Q[horizon_length]@(z[:,horizon_length]-Z[:,horizon_length])\n",
        "    for i in range(horizon_length):\n",
        "        #J=J+0.5*((u[:,i]-U[:,i]).T)@ R @(u[:,i]-U[:,i])+0.5*((z[:,i]-Z[:,i]).T)@ Q[i] @(z[:,i]-Z[:,i])+2*((z[:,i]-z_bar[:,i]).T)@Q[i]@(z[:,i]-Z[:,i])+2*((u[:,i]-u_bar[:,i]).T)@R@(u[:,i]-U[:,i])    \n",
        "        q1=2*(Z[:,i]-z_bar[:,i]).T@Q[i]\n",
        "        q.append(q1)\n",
        "        r1=2*(U[:,i]-u_bar[:,i]).T@R\n",
        "        r.append(r1)\n",
        "    return q,r"
      ],
      "metadata": {
        "id": "r44U8lBaW7VF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=1\n",
        "horizon_length = 40\n",
        "Q2=0.001*np.eye(14)\n",
        "Q1=20*np.eye(14)\n",
        "for i in range(7,14):\n",
        "    Q1[i,i]=0.1\n",
        "Q=[]\n",
        "for i in range(horizon_length+1):\n",
        "    if (i==horizon_length):\n",
        "        Q.append(Q1)\n",
        "    else:\n",
        "        Q.append(Q2)\n",
        "\n",
        "R=0.001*np.eye(7)\n",
        "sigma=0\n",
        "C=np.eye(14)\n",
        "Sigma=1*np.eye(14)\n",
        "\n",
        "\n",
        "u_bar = np.zeros([7, horizon_length])\n",
        "U = np.zeros([7, horizon_length])\n",
        "Z = np.zeros([14,horizon_length+1])\n",
        "u= np.zeros([7, horizon_length])\n",
        "    \n",
        "      \n",
        "z_bar=np.zeros([14,horizon_length+1])\n",
        "z_bar[0:7,horizon_length]=np.array([-1.8336430453226058, -1.4900804580013904, -0.33423961967431476, -1.108836361791467, 0.7815563306180698, 0.5042580934421823, 1.1407696303315698])\n",
        "z_bar[0:7,horizon_length-1]=np.array([-1.8336430453226058, -1.4900804580013904, -0.33423961967431476, -1.108836361791467, 0.7815563306180698, 0.5042580934421823, 1.1407696303315698])"
      ],
      "metadata": {
        "id": "Mev1GLO2WR7g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}